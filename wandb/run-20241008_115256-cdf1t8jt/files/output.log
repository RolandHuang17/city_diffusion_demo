10/08/2024 11:52:57 - INFO - __main__ - ***** Running training *****
10/08/2024 11:52:57 - INFO - __main__ -   Num examples = 5228
10/08/2024 11:52:57 - INFO - __main__ -   Num Epochs = 12
10/08/2024 11:52:57 - INFO - __main__ -   Instantaneous batch size per device = 1
10/08/2024 11:52:57 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 4
10/08/2024 11:52:57 - INFO - __main__ -   Gradient Accumulation steps = 4
10/08/2024 11:52:57 - INFO - __main__ -   Total optimization steps = 15000
Steps:   3%| | 500/15000 [01:11<33:25,  7.23it/s, lr10/08/2024 11:54:08 - INFO - accelerate.accelerator - Saving current state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-500
[2024-10-08 11:52:59,007] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648
[2024-10-08 11:52:59,183] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2147483648, reducing to 1073741824
[2024-10-08 11:52:59,353] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1073741824, reducing to 536870912
[2024-10-08 11:52:59,524] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 536870912, reducing to 268435456
[2024-10-08 11:52:59,670] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 268435456, reducing to 134217728
[2024-10-08 11:52:59,792] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 134217728, reducing to 67108864
[2024-10-08 11:52:59,918] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 67108864, reducing to 33554432
[2024-10-08 11:53:00,044] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 33554432, reducing to 16777216
[2024-10-08 11:53:00,573] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16777216, reducing to 8388608
[2024-10-08 11:53:00,841] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8388608, reducing to 4194304
[2024-10-08 11:53:01,262] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4194304, reducing to 2097152
[2024-10-08 11:53:03,315] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2097152, reducing to 1048576
10/08/2024 11:54:08 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2024-10-08 11:54:08,617] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-10-08 11:54:09,517] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: root/autodl-tmp/sddata/finetune/lora/city/checkpoint-500/pytorch_model/mp_rank_00_model_states.pt
[2024-10-08 11:54:09,517] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-500/pytorch_model/mp_rank_00_model_states.pt...
[2024-10-08 11:54:28,513] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-500/pytorch_model/mp_rank_00_model_states.pt.
[2024-10-08 11:54:28,579] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-500/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-10-08 11:54:28,655] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-500/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-10-08 11:54:28,655] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-500/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-10-08 11:54:28,656] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
10/08/2024 11:54:28 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir root/autodl-tmp/sddata/finetune/lora/city/checkpoint-500/pytorch_model
10/08/2024 11:54:28 - INFO - accelerate.checkpointing - Scheduler state saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-500/scheduler.bin
10/08/2024 11:54:28 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-500/sampler.bin
10/08/2024 11:54:28 - INFO - accelerate.checkpointing - Random states saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-500/random_states_0.pkl
Model weights saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-500/pytorch_lora_weights.safetensors
10/08/2024 11:54:28 - INFO - __main__ - Saved state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-500
Steps:   7%| | 1000/15000 [02:41<32:13,  7.24it/s, l10/08/2024 11:55:38 - INFO - accelerate.accelerator - Saving current state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-1000
[2024-10-08 11:54:43,874] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1048576, reducing to 524288
10/08/2024 11:55:38 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2024-10-08 11:55:38,703] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-10-08 11:55:39,584] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: root/autodl-tmp/sddata/finetune/lora/city/checkpoint-1000/pytorch_model/mp_rank_00_model_states.pt
[2024-10-08 11:55:39,585] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-1000/pytorch_model/mp_rank_00_model_states.pt...
[2024-10-08 11:56:00,006] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-1000/pytorch_model/mp_rank_00_model_states.pt.
[2024-10-08 11:56:00,075] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-1000/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-10-08 11:56:00,119] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-1000/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-10-08 11:56:00,120] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-1000/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-10-08 11:56:00,120] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
10/08/2024 11:56:00 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir root/autodl-tmp/sddata/finetune/lora/city/checkpoint-1000/pytorch_model
10/08/2024 11:56:00 - INFO - accelerate.checkpointing - Scheduler state saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-1000/scheduler.bin
10/08/2024 11:56:00 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-1000/sampler.bin
10/08/2024 11:56:00 - INFO - accelerate.checkpointing - Random states saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-1000/random_states_0.pkl
Model weights saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-1000/pytorch_lora_weights.safetensors
10/08/2024 11:56:00 - INFO - __main__ - Saved state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-1000
Steps:  10%| | 1500/15000 [04:13<31:42,  7.10it/s, l10/08/2024 11:57:10 - INFO - accelerate.accelerator - Saving current state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-1500
10/08/2024 11:57:10 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2024-10-08 11:57:10,330] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-10-08 11:57:11,223] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: root/autodl-tmp/sddata/finetune/lora/city/checkpoint-1500/pytorch_model/mp_rank_00_model_states.pt
[2024-10-08 11:57:11,224] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-1500/pytorch_model/mp_rank_00_model_states.pt...
[2024-10-08 11:57:36,461] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-1500/pytorch_model/mp_rank_00_model_states.pt.
[2024-10-08 11:57:36,542] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-1500/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-10-08 11:57:36,639] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-1500/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-10-08 11:57:36,639] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-1500/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-10-08 11:57:36,639] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
10/08/2024 11:57:36 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir root/autodl-tmp/sddata/finetune/lora/city/checkpoint-1500/pytorch_model
10/08/2024 11:57:36 - INFO - accelerate.checkpointing - Scheduler state saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-1500/scheduler.bin
10/08/2024 11:57:36 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-1500/sampler.bin
10/08/2024 11:57:36 - INFO - accelerate.checkpointing - Random states saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-1500/random_states_0.pkl
Model weights saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-1500/pytorch_lora_weights.safetensors
10/08/2024 11:57:36 - INFO - __main__ - Saved state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-1500
Steps:  13%|▏| 2000/15000 [05:49<29:22,  7.38it/s, l10/08/2024 11:58:46 - INFO - accelerate.accelerator - Saving current state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-2000
10/08/2024 11:58:46 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2024-10-08 11:58:46,440] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-10-08 11:58:47,293] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: root/autodl-tmp/sddata/finetune/lora/city/checkpoint-2000/pytorch_model/mp_rank_00_model_states.pt
[2024-10-08 11:58:47,294] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-2000/pytorch_model/mp_rank_00_model_states.pt...
[2024-10-08 11:59:10,952] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-2000/pytorch_model/mp_rank_00_model_states.pt.
[2024-10-08 11:59:11,018] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-2000/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-10-08 11:59:11,116] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-2000/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-10-08 11:59:11,117] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-2000/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-10-08 11:59:11,117] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
10/08/2024 11:59:11 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir root/autodl-tmp/sddata/finetune/lora/city/checkpoint-2000/pytorch_model
10/08/2024 11:59:11 - INFO - accelerate.checkpointing - Scheduler state saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-2000/scheduler.bin
10/08/2024 11:59:11 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-2000/sampler.bin
10/08/2024 11:59:11 - INFO - accelerate.checkpointing - Random states saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-2000/random_states_0.pkl
Model weights saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-2000/pytorch_lora_weights.safetensors
10/08/2024 11:59:11 - INFO - __main__ - Saved state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-2000
Steps:  17%|▏| 2500/15000 [07:23<28:30,  7.31it/s, l10/08/2024 12:00:20 - INFO - accelerate.accelerator - Saving current state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-2500
10/08/2024 12:00:20 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2024-10-08 12:00:20,852] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-10-08 12:00:21,723] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: root/autodl-tmp/sddata/finetune/lora/city/checkpoint-2500/pytorch_model/mp_rank_00_model_states.pt
[2024-10-08 12:00:21,723] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-2500/pytorch_model/mp_rank_00_model_states.pt...
[2024-10-08 12:00:40,539] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-2500/pytorch_model/mp_rank_00_model_states.pt.
[2024-10-08 12:00:40,662] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-2500/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-10-08 12:00:40,692] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-2500/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-10-08 12:00:40,693] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-2500/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-10-08 12:00:40,693] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
10/08/2024 12:00:40 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir root/autodl-tmp/sddata/finetune/lora/city/checkpoint-2500/pytorch_model
10/08/2024 12:00:40 - INFO - accelerate.checkpointing - Scheduler state saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-2500/scheduler.bin
10/08/2024 12:00:40 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-2500/sampler.bin
10/08/2024 12:00:40 - INFO - accelerate.checkpointing - Random states saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-2500/random_states_0.pkl
Model weights saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-2500/pytorch_lora_weights.safetensors
10/08/2024 12:00:40 - INFO - __main__ - Saved state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-2500
Steps:  20%|▏| 3000/15000 [08:53<27:10,  7.36it/s, l10/08/2024 12:01:50 - INFO - accelerate.accelerator - Saving current state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-3000
[2024-10-08 12:00:56,448] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2097152, reducing to 1048576
10/08/2024 12:01:50 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2024-10-08 12:01:50,895] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-10-08 12:01:51,749] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: root/autodl-tmp/sddata/finetune/lora/city/checkpoint-3000/pytorch_model/mp_rank_00_model_states.pt
[2024-10-08 12:01:51,750] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-3000/pytorch_model/mp_rank_00_model_states.pt...
[2024-10-08 12:02:11,298] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-3000/pytorch_model/mp_rank_00_model_states.pt.
[2024-10-08 12:02:11,363] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-3000/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-10-08 12:02:11,401] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-3000/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-10-08 12:02:11,467] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-3000/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-10-08 12:02:11,467] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
10/08/2024 12:02:11 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir root/autodl-tmp/sddata/finetune/lora/city/checkpoint-3000/pytorch_model
10/08/2024 12:02:11 - INFO - accelerate.checkpointing - Scheduler state saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-3000/scheduler.bin
10/08/2024 12:02:11 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-3000/sampler.bin
10/08/2024 12:02:11 - INFO - accelerate.checkpointing - Random states saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-3000/random_states_0.pkl
Model weights saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-3000/pytorch_lora_weights.safetensors
10/08/2024 12:02:11 - INFO - __main__ - Saved state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-3000
Steps:  23%|▏| 3500/15000 [10:24<26:41,  7.18it/s, l10/08/2024 12:03:21 - INFO - accelerate.accelerator - Saving current state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-3500
10/08/2024 12:03:21 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2024-10-08 12:03:21,402] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-10-08 12:03:22,281] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: root/autodl-tmp/sddata/finetune/lora/city/checkpoint-3500/pytorch_model/mp_rank_00_model_states.pt
[2024-10-08 12:03:22,281] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-3500/pytorch_model/mp_rank_00_model_states.pt...
[2024-10-08 12:03:40,974] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-3500/pytorch_model/mp_rank_00_model_states.pt.
[2024-10-08 12:03:41,090] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-3500/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-10-08 12:03:41,189] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-3500/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-10-08 12:03:41,189] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-3500/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-10-08 12:03:41,190] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
10/08/2024 12:03:41 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir root/autodl-tmp/sddata/finetune/lora/city/checkpoint-3500/pytorch_model
10/08/2024 12:03:41 - INFO - accelerate.checkpointing - Scheduler state saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-3500/scheduler.bin
10/08/2024 12:03:41 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-3500/sampler.bin
10/08/2024 12:03:41 - INFO - accelerate.checkpointing - Random states saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-3500/random_states_0.pkl
Model weights saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-3500/pytorch_lora_weights.safetensors
10/08/2024 12:03:41 - INFO - __main__ - Saved state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-3500
Steps:  27%|▎| 4000/15000 [11:55<24:34,  7.46it/s, l10/08/2024 12:04:52 - INFO - accelerate.accelerator - Saving current state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-4000
[2024-10-08 12:03:59,123] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2097152, reducing to 1048576
10/08/2024 12:04:52 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2024-10-08 12:04:52,605] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-10-08 12:04:53,456] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: root/autodl-tmp/sddata/finetune/lora/city/checkpoint-4000/pytorch_model/mp_rank_00_model_states.pt
[2024-10-08 12:04:53,456] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-4000/pytorch_model/mp_rank_00_model_states.pt...
[2024-10-08 12:05:11,417] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-4000/pytorch_model/mp_rank_00_model_states.pt.
[2024-10-08 12:05:11,517] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-4000/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-10-08 12:05:11,550] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-4000/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-10-08 12:05:11,550] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-4000/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-10-08 12:05:11,551] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
10/08/2024 12:05:11 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir root/autodl-tmp/sddata/finetune/lora/city/checkpoint-4000/pytorch_model
10/08/2024 12:05:11 - INFO - accelerate.checkpointing - Scheduler state saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-4000/scheduler.bin
10/08/2024 12:05:11 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-4000/sampler.bin
10/08/2024 12:05:11 - INFO - accelerate.checkpointing - Random states saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-4000/random_states_0.pkl
Model weights saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-4000/pytorch_lora_weights.safetensors
10/08/2024 12:05:11 - INFO - __main__ - Saved state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-4000
Steps:  30%|▎| 4500/15000 [13:25<23:49,  7.35it/s, l10/08/2024 12:06:22 - INFO - accelerate.accelerator - Saving current state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-4500
10/08/2024 12:06:22 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2024-10-08 12:06:22,403] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-10-08 12:06:23,277] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: root/autodl-tmp/sddata/finetune/lora/city/checkpoint-4500/pytorch_model/mp_rank_00_model_states.pt
[2024-10-08 12:06:23,277] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-4500/pytorch_model/mp_rank_00_model_states.pt...
[2024-10-08 12:06:41,189] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-4500/pytorch_model/mp_rank_00_model_states.pt.
[2024-10-08 12:06:41,308] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-4500/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-10-08 12:06:41,344] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-4500/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-10-08 12:06:41,345] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-4500/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-10-08 12:06:41,345] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
10/08/2024 12:06:41 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir root/autodl-tmp/sddata/finetune/lora/city/checkpoint-4500/pytorch_model
10/08/2024 12:06:41 - INFO - accelerate.checkpointing - Scheduler state saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-4500/scheduler.bin
10/08/2024 12:06:41 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-4500/sampler.bin
10/08/2024 12:06:41 - INFO - accelerate.checkpointing - Random states saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-4500/random_states_0.pkl
Model weights saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-4500/pytorch_lora_weights.safetensors
10/08/2024 12:06:41 - INFO - __main__ - Saved state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-4500
Steps:  33%|▎| 5000/15000 [14:54<23:26,  7.11it/s, l10/08/2024 12:07:51 - INFO - accelerate.accelerator - Saving current state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-5000
[2024-10-08 12:07:12,684] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2097152, reducing to 1048576
[2024-10-08 12:07:50,732] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1048576, reducing to 524288
10/08/2024 12:07:51 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2024-10-08 12:07:51,297] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-10-08 12:07:52,180] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: root/autodl-tmp/sddata/finetune/lora/city/checkpoint-5000/pytorch_model/mp_rank_00_model_states.pt
[2024-10-08 12:07:52,180] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-5000/pytorch_model/mp_rank_00_model_states.pt...
[2024-10-08 12:08:10,206] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-5000/pytorch_model/mp_rank_00_model_states.pt.
[2024-10-08 12:08:10,294] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-5000/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-10-08 12:08:10,331] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-5000/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-10-08 12:08:10,332] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-5000/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-10-08 12:08:10,332] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
10/08/2024 12:08:10 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir root/autodl-tmp/sddata/finetune/lora/city/checkpoint-5000/pytorch_model
10/08/2024 12:08:10 - INFO - accelerate.checkpointing - Scheduler state saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-5000/scheduler.bin
10/08/2024 12:08:10 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-5000/sampler.bin
10/08/2024 12:08:10 - INFO - accelerate.checkpointing - Random states saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-5000/random_states_0.pkl
Model weights saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-5000/pytorch_lora_weights.safetensors
10/08/2024 12:08:10 - INFO - __main__ - Saved state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-5000
model_index.json: 100%|█| 481/481 [00:00<00:00, 47.1
{'requires_safety_checker', 'image_encoder'} was not found in config. Values will be initialized to default values.
                                                    Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of bguisard/stable-diffusion-nano-2-1.
Loading pipeline components...:   0%| | 0/5 [00:00<?{'force_upcast', 'mid_block_add_attention', 'latents_std', 'latents_mean', 'use_post_quant_conv', 'use_quant_conv', 'shift_factor'} was not found in config. Values will be initialized to default values.
Loaded vae as AutoencoderKL from `vae` subfolder of bguisard/stable-diffusion-nano-2-1.
                                                    {'timestep_spacing'} was not found in config. Values will be initialized to default values.
Loaded scheduler as PNDMScheduler from `scheduler` subfolder of bguisard/stable-diffusion-nano-2-1.
/root/miniconda3/envs/diff/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of bguisard/stable-diffusion-nano-2-1.
Loading pipeline components...: 100%|█| 5/5 [00:00<0
You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .
10/08/2024 12:08:45 - INFO - __main__ - Running validation...
 Generating 4 images with prompt: Totoro.
Steps:  37%|▎| 5500/15000 [16:33<21:28,  7.37it/s, l10/08/2024 12:09:30 - INFO - accelerate.accelerator - Saving current state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-5500
10/08/2024 12:09:30 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2024-10-08 12:09:30,280] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-10-08 12:09:31,288] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: root/autodl-tmp/sddata/finetune/lora/city/checkpoint-5500/pytorch_model/mp_rank_00_model_states.pt
[2024-10-08 12:09:31,288] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-5500/pytorch_model/mp_rank_00_model_states.pt...
[2024-10-08 12:09:49,365] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-5500/pytorch_model/mp_rank_00_model_states.pt.
[2024-10-08 12:09:49,457] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-5500/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-10-08 12:09:49,493] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-5500/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-10-08 12:09:49,494] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-5500/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-10-08 12:09:49,494] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
10/08/2024 12:09:49 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir root/autodl-tmp/sddata/finetune/lora/city/checkpoint-5500/pytorch_model
10/08/2024 12:09:49 - INFO - accelerate.checkpointing - Scheduler state saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-5500/scheduler.bin
10/08/2024 12:09:49 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-5500/sampler.bin
10/08/2024 12:09:49 - INFO - accelerate.checkpointing - Random states saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-5500/random_states_0.pkl
Model weights saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-5500/pytorch_lora_weights.safetensors
10/08/2024 12:09:49 - INFO - __main__ - Saved state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-5500
Steps:  40%|▍| 6000/15000 [18:02<20:56,  7.16it/s, l10/08/2024 12:10:59 - INFO - accelerate.accelerator - Saving current state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-6000
10/08/2024 12:10:59 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2024-10-08 12:10:59,548] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-10-08 12:11:00,456] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: root/autodl-tmp/sddata/finetune/lora/city/checkpoint-6000/pytorch_model/mp_rank_00_model_states.pt
[2024-10-08 12:11:00,456] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-6000/pytorch_model/mp_rank_00_model_states.pt...
[2024-10-08 12:11:17,887] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-6000/pytorch_model/mp_rank_00_model_states.pt.
[2024-10-08 12:11:17,986] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-6000/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-10-08 12:11:18,003] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-6000/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-10-08 12:11:18,004] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-6000/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-10-08 12:11:18,004] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
10/08/2024 12:11:18 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir root/autodl-tmp/sddata/finetune/lora/city/checkpoint-6000/pytorch_model
10/08/2024 12:11:18 - INFO - accelerate.checkpointing - Scheduler state saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-6000/scheduler.bin
10/08/2024 12:11:18 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-6000/sampler.bin
10/08/2024 12:11:18 - INFO - accelerate.checkpointing - Random states saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-6000/random_states_0.pkl
Model weights saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-6000/pytorch_lora_weights.safetensors
10/08/2024 12:11:18 - INFO - __main__ - Saved state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-6000
Steps:  43%|▍| 6500/15000 [19:30<19:35,  7.23it/s, l10/08/2024 12:12:28 - INFO - accelerate.accelerator - Saving current state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-6500
10/08/2024 12:12:28 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2024-10-08 12:12:28,064] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-10-08 12:12:28,953] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: root/autodl-tmp/sddata/finetune/lora/city/checkpoint-6500/pytorch_model/mp_rank_00_model_states.pt
[2024-10-08 12:12:28,953] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-6500/pytorch_model/mp_rank_00_model_states.pt...
[2024-10-08 12:12:46,633] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-6500/pytorch_model/mp_rank_00_model_states.pt.
[2024-10-08 12:12:46,735] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-6500/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-10-08 12:12:46,772] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-6500/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-10-08 12:12:46,772] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-6500/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-10-08 12:12:46,772] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
10/08/2024 12:12:46 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir root/autodl-tmp/sddata/finetune/lora/city/checkpoint-6500/pytorch_model
10/08/2024 12:12:46 - INFO - accelerate.checkpointing - Scheduler state saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-6500/scheduler.bin
10/08/2024 12:12:46 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-6500/sampler.bin
10/08/2024 12:12:46 - INFO - accelerate.checkpointing - Random states saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-6500/random_states_0.pkl
Model weights saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-6500/pytorch_lora_weights.safetensors
10/08/2024 12:12:46 - INFO - __main__ - Saved state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-6500
Steps:  47%|▍| 7000/15000 [21:00<22:16,  5.99it/s, l10/08/2024 12:13:57 - INFO - accelerate.accelerator - Saving current state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-7000
10/08/2024 12:13:57 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2024-10-08 12:13:57,472] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-10-08 12:13:58,382] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: root/autodl-tmp/sddata/finetune/lora/city/checkpoint-7000/pytorch_model/mp_rank_00_model_states.pt
[2024-10-08 12:13:58,382] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-7000/pytorch_model/mp_rank_00_model_states.pt...
[2024-10-08 12:14:16,805] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-7000/pytorch_model/mp_rank_00_model_states.pt.
[2024-10-08 12:14:16,898] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-7000/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-10-08 12:14:16,931] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-7000/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-10-08 12:14:16,932] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-7000/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-10-08 12:14:16,932] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
10/08/2024 12:14:16 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir root/autodl-tmp/sddata/finetune/lora/city/checkpoint-7000/pytorch_model
10/08/2024 12:14:16 - INFO - accelerate.checkpointing - Scheduler state saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-7000/scheduler.bin
10/08/2024 12:14:16 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-7000/sampler.bin
10/08/2024 12:14:16 - INFO - accelerate.checkpointing - Random states saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-7000/random_states_0.pkl
Model weights saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-7000/pytorch_lora_weights.safetensors
10/08/2024 12:14:16 - INFO - __main__ - Saved state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-7000
Steps:  50%|▌| 7500/15000 [22:29<17:15,  7.24it/s, l10/08/2024 12:15:26 - INFO - accelerate.accelerator - Saving current state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-7500
[2024-10-08 12:14:27,557] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2097152, reducing to 1048576
10/08/2024 12:15:26 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2024-10-08 12:15:26,714] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-10-08 12:15:27,594] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: root/autodl-tmp/sddata/finetune/lora/city/checkpoint-7500/pytorch_model/mp_rank_00_model_states.pt
[2024-10-08 12:15:27,594] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-7500/pytorch_model/mp_rank_00_model_states.pt...
[2024-10-08 12:15:45,218] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-7500/pytorch_model/mp_rank_00_model_states.pt.
[2024-10-08 12:15:45,313] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-7500/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-10-08 12:15:45,346] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-7500/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-10-08 12:15:45,347] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-7500/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-10-08 12:15:45,347] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
10/08/2024 12:15:45 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir root/autodl-tmp/sddata/finetune/lora/city/checkpoint-7500/pytorch_model
10/08/2024 12:15:45 - INFO - accelerate.checkpointing - Scheduler state saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-7500/scheduler.bin
10/08/2024 12:15:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-7500/sampler.bin
10/08/2024 12:15:45 - INFO - accelerate.checkpointing - Random states saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-7500/random_states_0.pkl
Model weights saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-7500/pytorch_lora_weights.safetensors
10/08/2024 12:15:45 - INFO - __main__ - Saved state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-7500
Steps:  53%|▌| 8000/15000 [23:58<16:12,  7.20it/s, l10/08/2024 12:16:55 - INFO - accelerate.accelerator - Saving current state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-8000
[2024-10-08 12:16:53,517] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1048576, reducing to 524288
10/08/2024 12:16:55 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2024-10-08 12:16:55,314] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-10-08 12:16:56,176] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: root/autodl-tmp/sddata/finetune/lora/city/checkpoint-8000/pytorch_model/mp_rank_00_model_states.pt
[2024-10-08 12:16:56,176] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-8000/pytorch_model/mp_rank_00_model_states.pt...
[2024-10-08 12:17:14,129] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-8000/pytorch_model/mp_rank_00_model_states.pt.
[2024-10-08 12:17:14,221] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-8000/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-10-08 12:17:14,238] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-8000/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-10-08 12:17:14,239] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-8000/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-10-08 12:17:14,239] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
10/08/2024 12:17:14 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir root/autodl-tmp/sddata/finetune/lora/city/checkpoint-8000/pytorch_model
10/08/2024 12:17:14 - INFO - accelerate.checkpointing - Scheduler state saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-8000/scheduler.bin
10/08/2024 12:17:14 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-8000/sampler.bin
10/08/2024 12:17:14 - INFO - accelerate.checkpointing - Random states saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-8000/random_states_0.pkl
Model weights saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-8000/pytorch_lora_weights.safetensors
10/08/2024 12:17:14 - INFO - __main__ - Saved state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-8000
Steps:  57%|▌| 8500/15000 [25:28<15:01,  7.21it/s, l10/08/2024 12:18:25 - INFO - accelerate.accelerator - Saving current state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-8500
10/08/2024 12:18:25 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2024-10-08 12:18:25,271] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-10-08 12:18:26,141] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: root/autodl-tmp/sddata/finetune/lora/city/checkpoint-8500/pytorch_model/mp_rank_00_model_states.pt
[2024-10-08 12:18:26,142] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-8500/pytorch_model/mp_rank_00_model_states.pt...
[2024-10-08 12:18:43,688] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-8500/pytorch_model/mp_rank_00_model_states.pt.
[2024-10-08 12:18:43,779] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-8500/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-10-08 12:18:43,813] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-8500/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-10-08 12:18:43,814] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-8500/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-10-08 12:18:43,814] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
10/08/2024 12:18:43 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir root/autodl-tmp/sddata/finetune/lora/city/checkpoint-8500/pytorch_model
10/08/2024 12:18:43 - INFO - accelerate.checkpointing - Scheduler state saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-8500/scheduler.bin
10/08/2024 12:18:43 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-8500/sampler.bin
10/08/2024 12:18:43 - INFO - accelerate.checkpointing - Random states saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-8500/random_states_0.pkl
Model weights saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-8500/pytorch_lora_weights.safetensors
10/08/2024 12:18:43 - INFO - __main__ - Saved state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-8500
Steps:  60%|▌| 9000/15000 [26:57<17:13,  5.81it/s, l10/08/2024 12:19:54 - INFO - accelerate.accelerator - Saving current state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-9000
10/08/2024 12:19:54 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2024-10-08 12:19:54,620] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-10-08 12:19:55,638] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: root/autodl-tmp/sddata/finetune/lora/city/checkpoint-9000/pytorch_model/mp_rank_00_model_states.pt
[2024-10-08 12:19:55,638] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-9000/pytorch_model/mp_rank_00_model_states.pt...
[2024-10-08 12:20:13,695] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-9000/pytorch_model/mp_rank_00_model_states.pt.
[2024-10-08 12:20:13,787] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-9000/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-10-08 12:20:13,806] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-9000/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-10-08 12:20:13,806] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-9000/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-10-08 12:20:13,807] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
10/08/2024 12:20:13 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir root/autodl-tmp/sddata/finetune/lora/city/checkpoint-9000/pytorch_model
10/08/2024 12:20:13 - INFO - accelerate.checkpointing - Scheduler state saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-9000/scheduler.bin
10/08/2024 12:20:13 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-9000/sampler.bin
10/08/2024 12:20:13 - INFO - accelerate.checkpointing - Random states saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-9000/random_states_0.pkl
Model weights saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-9000/pytorch_lora_weights.safetensors
10/08/2024 12:20:13 - INFO - __main__ - Saved state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-9000
Steps:  63%|▋| 9500/15000 [28:28<12:31,  7.32it/s, l10/08/2024 12:21:25 - INFO - accelerate.accelerator - Saving current state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-9500
10/08/2024 12:21:25 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2024-10-08 12:21:25,299] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-10-08 12:21:26,141] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: root/autodl-tmp/sddata/finetune/lora/city/checkpoint-9500/pytorch_model/mp_rank_00_model_states.pt
[2024-10-08 12:21:26,141] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-9500/pytorch_model/mp_rank_00_model_states.pt...
[2024-10-08 12:21:43,962] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-9500/pytorch_model/mp_rank_00_model_states.pt.
[2024-10-08 12:21:44,003] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-9500/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-10-08 12:21:44,048] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-9500/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-10-08 12:21:44,048] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-9500/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-10-08 12:21:44,048] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
10/08/2024 12:21:44 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir root/autodl-tmp/sddata/finetune/lora/city/checkpoint-9500/pytorch_model
10/08/2024 12:21:44 - INFO - accelerate.checkpointing - Scheduler state saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-9500/scheduler.bin
10/08/2024 12:21:44 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-9500/sampler.bin
10/08/2024 12:21:44 - INFO - accelerate.checkpointing - Random states saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-9500/random_states_0.pkl
Model weights saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-9500/pytorch_lora_weights.safetensors
10/08/2024 12:21:44 - INFO - __main__ - Saved state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-9500
Steps:  67%|▋| 10000/15000 [29:56<11:13,  7.43it/s, 10/08/2024 12:22:53 - INFO - accelerate.accelerator - Saving current state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-10000
10/08/2024 12:22:53 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2024-10-08 12:22:53,924] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-10-08 12:22:54,769] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: root/autodl-tmp/sddata/finetune/lora/city/checkpoint-10000/pytorch_model/mp_rank_00_model_states.pt
[2024-10-08 12:22:54,769] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-10000/pytorch_model/mp_rank_00_model_states.pt...
[2024-10-08 12:23:12,981] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-10000/pytorch_model/mp_rank_00_model_states.pt.
[2024-10-08 12:23:13,073] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-10000/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-10-08 12:23:13,107] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-10000/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-10-08 12:23:13,107] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-10000/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-10-08 12:23:13,108] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
10/08/2024 12:23:13 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir root/autodl-tmp/sddata/finetune/lora/city/checkpoint-10000/pytorch_model
10/08/2024 12:23:13 - INFO - accelerate.checkpointing - Scheduler state saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-10000/scheduler.bin
10/08/2024 12:23:13 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-10000/sampler.bin
10/08/2024 12:23:13 - INFO - accelerate.checkpointing - Random states saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-10000/random_states_0.pkl
Model weights saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-10000/pytorch_lora_weights.safetensors
10/08/2024 12:23:13 - INFO - __main__ - Saved state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-10000
Steps:  70%|▋| 10500/15000 [31:27<10:24,  7.20it/s, 10/08/2024 12:24:24 - INFO - accelerate.accelerator - Saving current state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-10500
[2024-10-08 12:24:05,522] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2097152, reducing to 1048576
10/08/2024 12:24:24 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2024-10-08 12:24:24,456] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-10-08 12:24:25,334] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: root/autodl-tmp/sddata/finetune/lora/city/checkpoint-10500/pytorch_model/mp_rank_00_model_states.pt
[2024-10-08 12:24:25,334] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-10500/pytorch_model/mp_rank_00_model_states.pt...
[2024-10-08 12:24:43,792] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-10500/pytorch_model/mp_rank_00_model_states.pt.
[2024-10-08 12:24:43,883] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-10500/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-10-08 12:24:43,918] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-10500/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-10-08 12:24:43,918] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-10500/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-10-08 12:24:43,918] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
10/08/2024 12:24:43 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir root/autodl-tmp/sddata/finetune/lora/city/checkpoint-10500/pytorch_model
10/08/2024 12:24:43 - INFO - accelerate.checkpointing - Scheduler state saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-10500/scheduler.bin
10/08/2024 12:24:43 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-10500/sampler.bin
10/08/2024 12:24:43 - INFO - accelerate.checkpointing - Random states saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-10500/random_states_0.pkl
Model weights saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-10500/pytorch_lora_weights.safetensors
10/08/2024 12:24:43 - INFO - __main__ - Saved state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-10500
Steps:  73%|▋| 11000/15000 [32:57<09:19,  7.15it/s, 10/08/2024 12:25:54 - INFO - accelerate.accelerator - Saving current state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-11000
10/08/2024 12:25:54 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2024-10-08 12:25:54,726] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-10-08 12:25:55,594] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: root/autodl-tmp/sddata/finetune/lora/city/checkpoint-11000/pytorch_model/mp_rank_00_model_states.pt
[2024-10-08 12:25:55,594] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-11000/pytorch_model/mp_rank_00_model_states.pt...
[2024-10-08 12:26:13,468] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-11000/pytorch_model/mp_rank_00_model_states.pt.
[2024-10-08 12:26:13,559] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-11000/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-10-08 12:26:13,591] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-11000/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-10-08 12:26:13,592] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-11000/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-10-08 12:26:13,592] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
10/08/2024 12:26:13 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir root/autodl-tmp/sddata/finetune/lora/city/checkpoint-11000/pytorch_model
10/08/2024 12:26:13 - INFO - accelerate.checkpointing - Scheduler state saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-11000/scheduler.bin
10/08/2024 12:26:13 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-11000/sampler.bin
10/08/2024 12:26:13 - INFO - accelerate.checkpointing - Random states saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-11000/random_states_0.pkl
Model weights saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-11000/pytorch_lora_weights.safetensors
10/08/2024 12:26:13 - INFO - __main__ - Saved state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-11000
Steps:  77%|▊| 11500/15000 [34:25<07:56,  7.35it/s, 10/08/2024 12:27:23 - INFO - accelerate.accelerator - Saving current state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-11500
10/08/2024 12:27:23 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2024-10-08 12:27:23,076] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-10-08 12:27:23,916] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: root/autodl-tmp/sddata/finetune/lora/city/checkpoint-11500/pytorch_model/mp_rank_00_model_states.pt
[2024-10-08 12:27:23,916] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-11500/pytorch_model/mp_rank_00_model_states.pt...
[2024-10-08 12:27:41,915] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-11500/pytorch_model/mp_rank_00_model_states.pt.
[2024-10-08 12:27:41,960] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-11500/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-10-08 12:27:42,003] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-11500/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-10-08 12:27:42,005] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-11500/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-10-08 12:27:42,005] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
10/08/2024 12:27:42 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir root/autodl-tmp/sddata/finetune/lora/city/checkpoint-11500/pytorch_model
10/08/2024 12:27:42 - INFO - accelerate.checkpointing - Scheduler state saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-11500/scheduler.bin
10/08/2024 12:27:42 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-11500/sampler.bin
10/08/2024 12:27:42 - INFO - accelerate.checkpointing - Random states saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-11500/random_states_0.pkl
Model weights saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-11500/pytorch_lora_weights.safetensors
10/08/2024 12:27:42 - INFO - __main__ - Saved state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-11500
Steps:  80%|▊| 12000/15000 [35:55<07:00,  7.13it/s, 10/08/2024 12:28:52 - INFO - accelerate.accelerator - Saving current state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-12000
[2024-10-08 12:27:45,248] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2097152, reducing to 1048576
10/08/2024 12:28:52 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2024-10-08 12:28:52,627] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-10-08 12:28:53,522] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: root/autodl-tmp/sddata/finetune/lora/city/checkpoint-12000/pytorch_model/mp_rank_00_model_states.pt
[2024-10-08 12:28:53,523] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-12000/pytorch_model/mp_rank_00_model_states.pt...
[2024-10-08 12:29:11,491] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-12000/pytorch_model/mp_rank_00_model_states.pt.
[2024-10-08 12:29:11,580] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-12000/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-10-08 12:29:11,599] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-12000/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-10-08 12:29:11,600] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-12000/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-10-08 12:29:11,600] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
10/08/2024 12:29:11 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir root/autodl-tmp/sddata/finetune/lora/city/checkpoint-12000/pytorch_model
10/08/2024 12:29:11 - INFO - accelerate.checkpointing - Scheduler state saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-12000/scheduler.bin
10/08/2024 12:29:11 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-12000/sampler.bin
10/08/2024 12:29:11 - INFO - accelerate.checkpointing - Random states saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-12000/random_states_0.pkl
Model weights saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-12000/pytorch_lora_weights.safetensors
10/08/2024 12:29:11 - INFO - __main__ - Saved state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-12000
Steps:  83%|▊| 12500/15000 [37:24<05:49,  7.16it/s, 10/08/2024 12:30:21 - INFO - accelerate.accelerator - Saving current state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-12500
10/08/2024 12:30:21 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2024-10-08 12:30:21,530] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-10-08 12:30:22,399] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: root/autodl-tmp/sddata/finetune/lora/city/checkpoint-12500/pytorch_model/mp_rank_00_model_states.pt
[2024-10-08 12:30:22,399] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-12500/pytorch_model/mp_rank_00_model_states.pt...
[2024-10-08 12:30:40,090] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-12500/pytorch_model/mp_rank_00_model_states.pt.
[2024-10-08 12:30:40,137] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-12500/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-10-08 12:30:40,161] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-12500/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-10-08 12:30:40,162] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-12500/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-10-08 12:30:40,162] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
10/08/2024 12:30:40 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir root/autodl-tmp/sddata/finetune/lora/city/checkpoint-12500/pytorch_model
10/08/2024 12:30:40 - INFO - accelerate.checkpointing - Scheduler state saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-12500/scheduler.bin
10/08/2024 12:30:40 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-12500/sampler.bin
10/08/2024 12:30:40 - INFO - accelerate.checkpointing - Random states saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-12500/random_states_0.pkl
Model weights saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-12500/pytorch_lora_weights.safetensors
10/08/2024 12:30:40 - INFO - __main__ - Saved state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-12500
Steps:  87%|▊| 13000/15000 [38:53<04:37,  7.20it/s, 10/08/2024 12:31:50 - INFO - accelerate.accelerator - Saving current state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-13000
[2024-10-08 12:31:02,991] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2097152, reducing to 1048576
10/08/2024 12:31:50 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2024-10-08 12:31:50,437] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-10-08 12:31:51,310] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: root/autodl-tmp/sddata/finetune/lora/city/checkpoint-13000/pytorch_model/mp_rank_00_model_states.pt
[2024-10-08 12:31:51,311] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-13000/pytorch_model/mp_rank_00_model_states.pt...
[2024-10-08 12:32:09,068] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-13000/pytorch_model/mp_rank_00_model_states.pt.
[2024-10-08 12:32:09,159] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-13000/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-10-08 12:32:09,193] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-13000/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-10-08 12:32:09,194] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-13000/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-10-08 12:32:09,194] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
10/08/2024 12:32:09 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir root/autodl-tmp/sddata/finetune/lora/city/checkpoint-13000/pytorch_model
10/08/2024 12:32:09 - INFO - accelerate.checkpointing - Scheduler state saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-13000/scheduler.bin
10/08/2024 12:32:09 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-13000/sampler.bin
10/08/2024 12:32:09 - INFO - accelerate.checkpointing - Random states saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-13000/random_states_0.pkl
Model weights saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-13000/pytorch_lora_weights.safetensors
10/08/2024 12:32:09 - INFO - __main__ - Saved state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-13000
Steps:  90%|▉| 13500/15000 [40:22<03:29,  7.16it/s, 10/08/2024 12:33:19 - INFO - accelerate.accelerator - Saving current state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-13500
10/08/2024 12:33:19 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2024-10-08 12:33:19,834] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-10-08 12:33:20,714] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: root/autodl-tmp/sddata/finetune/lora/city/checkpoint-13500/pytorch_model/mp_rank_00_model_states.pt
[2024-10-08 12:33:20,715] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-13500/pytorch_model/mp_rank_00_model_states.pt...
[2024-10-08 12:33:38,808] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-13500/pytorch_model/mp_rank_00_model_states.pt.
[2024-10-08 12:33:38,904] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-13500/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-10-08 12:33:38,936] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-13500/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-10-08 12:33:38,936] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-13500/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-10-08 12:33:38,937] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
10/08/2024 12:33:38 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir root/autodl-tmp/sddata/finetune/lora/city/checkpoint-13500/pytorch_model
10/08/2024 12:33:38 - INFO - accelerate.checkpointing - Scheduler state saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-13500/scheduler.bin
10/08/2024 12:33:38 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-13500/sampler.bin
10/08/2024 12:33:38 - INFO - accelerate.checkpointing - Random states saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-13500/random_states_0.pkl
Model weights saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-13500/pytorch_lora_weights.safetensors
10/08/2024 12:33:38 - INFO - __main__ - Saved state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-13500
Steps:  93%|▉| 14000/15000 [41:49<02:13,  7.51it/s, 10/08/2024 12:34:46 - INFO - accelerate.accelerator - Saving current state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-14000
10/08/2024 12:34:46 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2024-10-08 12:34:46,357] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-10-08 12:34:47,216] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: root/autodl-tmp/sddata/finetune/lora/city/checkpoint-14000/pytorch_model/mp_rank_00_model_states.pt
[2024-10-08 12:34:47,217] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-14000/pytorch_model/mp_rank_00_model_states.pt...
[2024-10-08 12:35:04,836] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-14000/pytorch_model/mp_rank_00_model_states.pt.
[2024-10-08 12:35:04,924] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-14000/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-10-08 12:35:04,956] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-14000/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-10-08 12:35:04,956] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-14000/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-10-08 12:35:04,956] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
10/08/2024 12:35:04 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir root/autodl-tmp/sddata/finetune/lora/city/checkpoint-14000/pytorch_model
10/08/2024 12:35:04 - INFO - accelerate.checkpointing - Scheduler state saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-14000/scheduler.bin
10/08/2024 12:35:04 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-14000/sampler.bin
10/08/2024 12:35:04 - INFO - accelerate.checkpointing - Random states saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-14000/random_states_0.pkl
Model weights saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-14000/pytorch_lora_weights.safetensors
10/08/2024 12:35:04 - INFO - __main__ - Saved state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-14000
Steps:  97%|▉| 14500/15000 [43:17<01:08,  7.29it/s, 10/08/2024 12:36:14 - INFO - accelerate.accelerator - Saving current state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-14500
[2024-10-08 12:35:17,661] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2097152, reducing to 1048576
10/08/2024 12:36:14 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2024-10-08 12:36:14,994] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-10-08 12:36:15,870] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: root/autodl-tmp/sddata/finetune/lora/city/checkpoint-14500/pytorch_model/mp_rank_00_model_states.pt
[2024-10-08 12:36:15,870] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-14500/pytorch_model/mp_rank_00_model_states.pt...
[2024-10-08 12:36:33,632] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-14500/pytorch_model/mp_rank_00_model_states.pt.
[2024-10-08 12:36:33,728] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-14500/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-10-08 12:36:33,761] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-14500/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-10-08 12:36:33,761] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-14500/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-10-08 12:36:33,762] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
10/08/2024 12:36:33 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir root/autodl-tmp/sddata/finetune/lora/city/checkpoint-14500/pytorch_model
10/08/2024 12:36:33 - INFO - accelerate.checkpointing - Scheduler state saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-14500/scheduler.bin
10/08/2024 12:36:33 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-14500/sampler.bin
10/08/2024 12:36:33 - INFO - accelerate.checkpointing - Random states saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-14500/random_states_0.pkl
Model weights saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-14500/pytorch_lora_weights.safetensors
10/08/2024 12:36:33 - INFO - __main__ - Saved state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-14500
Steps: 100%|█| 15000/15000 [44:46<00:00,  7.34it/s, 10/08/2024 12:37:43 - INFO - accelerate.accelerator - Saving current state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-15000
10/08/2024 12:37:43 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2024-10-08 12:37:43,712] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-10-08 12:37:44,558] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: root/autodl-tmp/sddata/finetune/lora/city/checkpoint-15000/pytorch_model/mp_rank_00_model_states.pt
[2024-10-08 12:37:44,558] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-15000/pytorch_model/mp_rank_00_model_states.pt...
[2024-10-08 12:38:02,584] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-15000/pytorch_model/mp_rank_00_model_states.pt.
[2024-10-08 12:38:02,675] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving root/autodl-tmp/sddata/finetune/lora/city/checkpoint-15000/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-10-08 12:38:02,707] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-15000/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-10-08 12:38:02,708] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved root/autodl-tmp/sddata/finetune/lora/city/checkpoint-15000/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-10-08 12:38:02,708] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
10/08/2024 12:38:02 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir root/autodl-tmp/sddata/finetune/lora/city/checkpoint-15000/pytorch_model
10/08/2024 12:38:02 - INFO - accelerate.checkpointing - Scheduler state saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-15000/scheduler.bin
10/08/2024 12:38:02 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-15000/sampler.bin
10/08/2024 12:38:02 - INFO - accelerate.checkpointing - Random states saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-15000/random_states_0.pkl
Model weights saved in root/autodl-tmp/sddata/finetune/lora/city/checkpoint-15000/pytorch_lora_weights.safetensors
10/08/2024 12:38:02 - INFO - __main__ - Saved state to root/autodl-tmp/sddata/finetune/lora/city/checkpoint-15000
Steps: 15009it [45:11,  1.07it/s, lr=2.15e-10, step_loss=0.122]Model weights saved in root/autodl-tmp/sddata/finetune/lora/city/pytorch_lora_weights.safetensors
{'requires_safety_checker', 'image_encoder'} was not found in config. Values will be initialized to default values.
                                                    {'conv_in_kernel', 'transformer_layers_per_block', 'addition_embed_type', 'conv_out_kernel', 'cross_attention_norm', 'addition_time_embed_dim', 'time_cond_proj_dim', 'resnet_skip_time_act', 'upcast_attention', 'attention_type', 'timestep_post_act', 'addition_embed_type_num_heads', 'mid_block_only_cross_attention', 'projection_class_embeddings_input_dim', 'dropout', 'resnet_out_scale_factor', 'num_attention_heads', 'encoder_hid_dim_type', 'time_embedding_type', 'class_embeddings_concat', 'class_embed_type', 'time_embedding_act_fn', 'time_embedding_dim', 'encoder_hid_dim', 'resnet_time_scale_shift', 'reverse_transformer_layers_per_block', 'mid_block_type'} was not found in config. Values will be initialized to default values.
Loaded unet as UNet2DConditionModel from `unet` subfolder of bguisard/stable-diffusion-nano-2-1.
                                                    Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of bguisard/stable-diffusion-nano-2-1.
Loading pipeline components...:  20%|▏| 1/5 [00:02<0{'force_upcast', 'mid_block_add_attention', 'latents_std', 'latents_mean', 'use_post_quant_conv', 'use_quant_conv', 'shift_factor'} was not found in config. Values will be initialized to default values.
Loaded vae as AutoencoderKL from `vae` subfolder of bguisard/stable-diffusion-nano-2-1.
                                                    {'timestep_spacing'} was not found in config. Values will be initialized to default values.
Loaded scheduler as PNDMScheduler from `scheduler` subfolder of bguisard/stable-diffusion-nano-2-1.
Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of bguisard/stable-diffusion-nano-2-1.
Loading pipeline components...: 100%|█| 5/5 [00:03<0
You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .
Loading unet.
10/08/2024 12:38:12 - INFO - __main__ - Running validation...
 Generating 4 images with prompt: Totoro.
random_states_0.pkl: 100%|█| 14.4k/14.4k [00:00<00:0
scheduler.bin: 100%|█| 1.00k/1.00k [00:00<00:00, 1.4
pytorch_lora_weights.safetensors: 100%|█| 1.70M/1.70
pytorch_lora_weights.safetensors: 100%|█| 1.70M/1.70
random_states_0.pkl: 100%|█| 14.4k/14.4k [00:00<00:0
scheduler.bin: 100%|█| 1.00k/1.00k [00:00<00:00, 3.2
pytorch_lora_weights.safetensors: 100%|█| 1.70M/1.70
zero_pp_rank_0_mp_rank_00_optim_states.pt: 100%|█| 9
zero_pp_rank_0_mp_rank_00_optim_states.pt: 100%|█| 9
random_states_0.pkl: 100%|█| 14.4k/14.4k [00:00<00:0
scheduler.bin: 100%|█| 1.00k/1.00k [00:00<00:00, 2.9
pytorch_lora_weights.safetensors: 100%|█| 1.70M/1.70
zero_pp_rank_0_mp_rank_00_optim_states.pt: 100%|█| 9
zero_pp_rank_0_mp_rank_00_optim_states.pt: 100%|█| 9
random_states_0.pkl: 100%|█| 14.4k/14.4k [00:00<00:0
scheduler.bin: 100%|█| 1.00k/1.00k [00:00<00:00, 2.8
pytorch_lora_weights.safetensors: 100%|█| 1.70M/1.70
zero_pp_rank_0_mp_rank_00_optim_states.pt: 100%|█| 9
random_states_0.pkl: 100%|█| 14.4k/14.4k [00:00<00:0
scheduler.bin: 100%|█| 1.00k/1.00k [00:00<00:00, 3.6
pytorch_lora_weights.safetensors: 100%|█| 1.70M/1.70
mp_rank_00_model_states.pt:  10%| | 336M/3.47G [20:0'(MaxRetryError("HTTPSConnectionPool(host='hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com', port=443): Max retries exceeded with url: /repos/fa/d9/fad929c7ce422826d6b5e86f6e73367545df88497599b59186ed519419c07ab5/a44d967051d043a0287c94ba8c6a5298be6e402a30cb809fe0c66b68f1c30c9f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20241008%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241008T044549Z&X-Amz-Expires=86400&X-Amz-Signature=1a5867eb1e4f9b173d60c0972178462829377398b8b5ac13afc75908424f4a65&X-Amz-SignedHeaders=host&partNumber=19&uploadId=HxoRi0BjEk4pdGeUvIURsWrACnOL4uWykyNA5REk4w3Nh8g_vpq2K1mIq5bN6eNy2WmkfEZ8OK3.WAp90VBdPtKyMDQzu5UylIY5MXTUsRXTBDXzjat2g6xjK.ruxzea&x-id=UploadPart (Caused by ProxyError('Unable to connect to proxy', RemoteDisconnected('Remote end closed connection without response')))"), '(Request ID: 821f101a-6eae-4c00-a69f-dcedd94fca8a)')' thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/fa/d9/fad929c7ce422826d6b5e86f6e73367545df88497599b59186ed519419c07ab5/a44d967051d043a0287c94ba8c6a5298be6e402a30cb809fe0c66b68f1c30c9f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20241008%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241008T044549Z&X-Amz-Expires=86400&X-Amz-Signature=1a5867eb1e4f9b173d60c0972178462829377398b8b5ac13afc75908424f4a65&X-Amz-SignedHeaders=host&partNumber=19&uploadId=HxoRi0BjEk4pdGeUvIURsWrACnOL4uWykyNA5REk4w3Nh8g_vpq2K1mIq5bN6eNy2WmkfEZ8OK3.WAp90VBdPtKyMDQzu5UylIY5MXTUsRXTBDXzjat2g6xjK.ruxzea&x-id=UploadPart
10/08/2024 13:06:35 - WARNING - huggingface_hub.utils._http - '(MaxRetryError("HTTPSConnectionPool(host='hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com', port=443): Max retries exceeded with url: /repos/fa/d9/fad929c7ce422826d6b5e86f6e73367545df88497599b59186ed519419c07ab5/a44d967051d043a0287c94ba8c6a5298be6e402a30cb809fe0c66b68f1c30c9f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20241008%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241008T044549Z&X-Amz-Expires=86400&X-Amz-Signature=1a5867eb1e4f9b173d60c0972178462829377398b8b5ac13afc75908424f4a65&X-Amz-SignedHeaders=host&partNumber=19&uploadId=HxoRi0BjEk4pdGeUvIURsWrACnOL4uWykyNA5REk4w3Nh8g_vpq2K1mIq5bN6eNy2WmkfEZ8OK3.WAp90VBdPtKyMDQzu5UylIY5MXTUsRXTBDXzjat2g6xjK.ruxzea&x-id=UploadPart (Caused by ProxyError('Unable to connect to proxy', RemoteDisconnected('Remote end closed connection without response')))"), '(Request ID: 821f101a-6eae-4c00-a69f-dcedd94fca8a)')' thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/fa/d9/fad929c7ce422826d6b5e86f6e73367545df88497599b59186ed519419c07ab5/a44d967051d043a0287c94ba8c6a5298be6e402a30cb809fe0c66b68f1c30c9f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20241008%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241008T044549Z&X-Amz-Expires=86400&X-Amz-Signature=1a5867eb1e4f9b173d60c0972178462829377398b8b5ac13afc75908424f4a65&X-Amz-SignedHeaders=host&partNumber=19&uploadId=HxoRi0BjEk4pdGeUvIURsWrACnOL4uWykyNA5REk4w3Nh8g_vpq2K1mIq5bN6eNy2WmkfEZ8OK3.WAp90VBdPtKyMDQzu5UylIY5MXTUsRXTBDXzjat2g6xjK.ruxzea&x-id=UploadPart
Retrying in 1s [Retry 1/5].
10/08/2024 13:06:35 - WARNING - huggingface_hub.utils._http - Retrying in 1s [Retry 1/5].
mp_rank_00_model_states.pt:  10%| | 336M/3.47G [20:3'(MaxRetryError("HTTPSConnectionPool(host='hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com', port=443): Max retries exceeded with url: /repos/fa/d9/fad929c7ce422826d6b5e86f6e73367545df88497599b59186ed519419c07ab5/19d04af0e3479ed569edf8736489cd0963626796f687a5297984208d5349fe06?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20241008%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241008T044548Z&X-Amz-Expires=86400&X-Amz-Signature=2a594d97f6f85bc6681191c710e04b805d0badbd8f3d3dddcef34596b014500f&X-Amz-SignedHeaders=host&partNumber=22&uploadId=H65wfJNm4sJ_XSqjhzNgq86Z4NSDQ8DIFt_ycyyraNyYDRgoXOIb9F3W6RkksECTK1pjmGcIoDDCsKmumrU7VNBRwxHr2_ZBPffzh64npBrD7mU8R65KjehiSJNnCGIn&x-id=UploadPart (Caused by ProxyError('Unable to connect to proxy', RemoteDisconnected('Remote end closed connection without response')))"), '(Request ID: b55a662d-0472-438e-ab12-37c4c52bfdf9)')' thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/fa/d9/fad929c7ce422826d6b5e86f6e73367545df88497599b59186ed519419c07ab5/19d04af0e3479ed569edf8736489cd0963626796f687a5297984208d5349fe06?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20241008%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241008T044548Z&X-Amz-Expires=86400&X-Amz-Signature=2a594d97f6f85bc6681191c710e04b805d0badbd8f3d3dddcef34596b014500f&X-Amz-SignedHeaders=host&partNumber=22&uploadId=H65wfJNm4sJ_XSqjhzNgq86Z4NSDQ8DIFt_ycyyraNyYDRgoXOIb9F3W6RkksECTK1pjmGcIoDDCsKmumrU7VNBRwxHr2_ZBPffzh64npBrD7mU8R65KjehiSJNnCGIn&x-id=UploadPart
10/08/2024 13:08:22 - WARNING - huggingface_hub.utils._http - '(MaxRetryError("HTTPSConnectionPool(host='hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com', port=443): Max retries exceeded with url: /repos/fa/d9/fad929c7ce422826d6b5e86f6e73367545df88497599b59186ed519419c07ab5/19d04af0e3479ed569edf8736489cd0963626796f687a5297984208d5349fe06?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20241008%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241008T044548Z&X-Amz-Expires=86400&X-Amz-Signature=2a594d97f6f85bc6681191c710e04b805d0badbd8f3d3dddcef34596b014500f&X-Amz-SignedHeaders=host&partNumber=22&uploadId=H65wfJNm4sJ_XSqjhzNgq86Z4NSDQ8DIFt_ycyyraNyYDRgoXOIb9F3W6RkksECTK1pjmGcIoDDCsKmumrU7VNBRwxHr2_ZBPffzh64npBrD7mU8R65KjehiSJNnCGIn&x-id=UploadPart (Caused by ProxyError('Unable to connect to proxy', RemoteDisconnected('Remote end closed connection without response')))"), '(Request ID: b55a662d-0472-438e-ab12-37c4c52bfdf9)')' thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/fa/d9/fad929c7ce422826d6b5e86f6e73367545df88497599b59186ed519419c07ab5/19d04af0e3479ed569edf8736489cd0963626796f687a5297984208d5349fe06?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20241008%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241008T044548Z&X-Amz-Expires=86400&X-Amz-Signature=2a594d97f6f85bc6681191c710e04b805d0badbd8f3d3dddcef34596b014500f&X-Amz-SignedHeaders=host&partNumber=22&uploadId=H65wfJNm4sJ_XSqjhzNgq86Z4NSDQ8DIFt_ycyyraNyYDRgoXOIb9F3W6RkksECTK1pjmGcIoDDCsKmumrU7VNBRwxHr2_ZBPffzh64npBrD7mU8R65KjehiSJNnCGIn&x-id=UploadPart
Retrying in 1s [Retry 1/5].  10%| | 352M/3.47G [21:5
10/08/2024 13:08:22 - WARNING - huggingface_hub.utils._http - Retrying in 1s [Retry 1/5].
mp_rank_00_model_states.pt:  10%| | 360M/3.47G [22:1'(MaxRetryError("HTTPSConnectionPool(host='hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com', port=443): Max retries exceeded with url: /repos/fa/d9/fad929c7ce422826d6b5e86f6e73367545df88497599b59186ed519419c07ab5/19d04af0e3479ed569edf8736489cd0963626796f687a5297984208d5349fe06?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20241008%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241008T044548Z&X-Amz-Expires=86400&X-Amz-Signature=1c8b846f99b19a24362f58831411618ffde3befad95224b6e6d316cc3201cecc&X-Amz-SignedHeaders=host&partNumber=23&uploadId=H65wfJNm4sJ_XSqjhzNgq86Z4NSDQ8DIFt_ycyyraNyYDRgoXOIb9F3W6RkksECTK1pjmGcIoDDCsKmumrU7VNBRwxHr2_ZBPffzh64npBrD7mU8R65KjehiSJNnCGIn&x-id=UploadPart (Caused by SSLError(SSLZeroReturnError(6, 'TLS/SSL connection has been closed (EOF) (_ssl.c:1133)')))"), '(Request ID: 7e8e1125-97b5-4b81-8475-2af2ee01a47b)')' thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/fa/d9/fad929c7ce422826d6b5e86f6e73367545df88497599b59186ed519419c07ab5/19d04af0e3479ed569edf8736489cd0963626796f687a5297984208d5349fe06?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20241008%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241008T044548Z&X-Amz-Expires=86400&X-Amz-Signature=1c8b846f99b19a24362f58831411618ffde3befad95224b6e6d316cc3201cecc&X-Amz-SignedHeaders=host&partNumber=23&uploadId=H65wfJNm4sJ_XSqjhzNgq86Z4NSDQ8DIFt_ycyyraNyYDRgoXOIb9F3W6RkksECTK1pjmGcIoDDCsKmumrU7VNBRwxHr2_ZBPffzh64npBrD7mU8R65KjehiSJNnCGIn&x-id=UploadPart
10/08/2024 13:10:56 - WARNING - huggingface_hub.utils._http - '(MaxRetryError("HTTPSConnectionPool(host='hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com', port=443): Max retries exceeded with url: /repos/fa/d9/fad929c7ce422826d6b5e86f6e73367545df88497599b59186ed519419c07ab5/19d04af0e3479ed569edf8736489cd0963626796f687a5297984208d5349fe06?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20241008%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241008T044548Z&X-Amz-Expires=86400&X-Amz-Signature=1c8b846f99b19a24362f58831411618ffde3befad95224b6e6d316cc3201cecc&X-Amz-SignedHeaders=host&partNumber=23&uploadId=H65wfJNm4sJ_XSqjhzNgq86Z4NSDQ8DIFt_ycyyraNyYDRgoXOIb9F3W6RkksECTK1pjmGcIoDDCsKmumrU7VNBRwxHr2_ZBPffzh64npBrD7mU8R65KjehiSJNnCGIn&x-id=UploadPart (Caused by SSLError(SSLZeroReturnError(6, 'TLS/SSL connection has been closed (EOF) (_ssl.c:1133)')))"), '(Request ID: 7e8e1125-97b5-4b81-8475-2af2ee01a47b)')' thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/fa/d9/fad929c7ce422826d6b5e86f6e73367545df88497599b59186ed519419c07ab5/19d04af0e3479ed569edf8736489cd0963626796f687a5297984208d5349fe06?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20241008%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241008T044548Z&X-Amz-Expires=86400&X-Amz-Signature=1c8b846f99b19a24362f58831411618ffde3befad95224b6e6d316cc3201cecc&X-Amz-SignedHeaders=host&partNumber=23&uploadId=H65wfJNm4sJ_XSqjhzNgq86Z4NSDQ8DIFt_ycyyraNyYDRgoXOIb9F3W6RkksECTK1pjmGcIoDDCsKmumrU7VNBRwxHr2_ZBPffzh64npBrD7mU8R65KjehiSJNnCGIn&x-id=UploadPart
Retrying in 1s [Retry 1/5].  11%| | 368M/3.47G [24:0
10/08/2024 13:10:56 - WARNING - huggingface_hub.utils._http - Retrying in 1s [Retry 1/5].
mp_rank_00_model_states.pt:  12%| | 399M/3.47G [24:3'(MaxRetryError("HTTPSConnectionPool(host='hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com', port=443): Max retries exceeded with url: /repos/fa/d9/fad929c7ce422826d6b5e86f6e73367545df88497599b59186ed519419c07ab5/68346dc23a374a7944af31adc5987294d2b1fd0d313fe8b32caf8f4ce8b1aff6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20241008%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241008T044548Z&X-Amz-Expires=86400&X-Amz-Signature=fe533b4490f5aa1d7e6027c06d4a393a5749d9fad17126920c296433dc19af25&X-Amz-SignedHeaders=host&partNumber=3&uploadId=Dz4n.pFEH3FoaxSDQuM3eaMH1LL3JsDVJPpjRAcdYD9E2ZQNUogR94xVvqpGzJgCasj2HXq7Ks3GJ_JL7tNPFxLzLjwrabuQRf9INIq0i3n6fwXkgEhRtpH9X8A7N92s&x-id=UploadPart (Caused by ProxyError('Unable to connect to proxy', RemoteDisconnected('Remote end closed connection without response')))"), '(Request ID: 57c85163-94af-4f6c-a7a0-c2f9bdae72d3)')' thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/fa/d9/fad929c7ce422826d6b5e86f6e73367545df88497599b59186ed519419c07ab5/68346dc23a374a7944af31adc5987294d2b1fd0d313fe8b32caf8f4ce8b1aff6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20241008%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241008T044548Z&X-Amz-Expires=86400&X-Amz-Signature=fe533b4490f5aa1d7e6027c06d4a393a5749d9fad17126920c296433dc19af25&X-Amz-SignedHeaders=host&partNumber=3&uploadId=Dz4n.pFEH3FoaxSDQuM3eaMH1LL3JsDVJPpjRAcdYD9E2ZQNUogR94xVvqpGzJgCasj2HXq7Ks3GJ_JL7tNPFxLzLjwrabuQRf9INIq0i3n6fwXkgEhRtpH9X8A7N92s&x-id=UploadPart
10/08/2024 13:11:09 - WARNING - huggingface_hub.utils._http - '(MaxRetryError("HTTPSConnectionPool(host='hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com', port=443): Max retries exceeded with url: /repos/fa/d9/fad929c7ce422826d6b5e86f6e73367545df88497599b59186ed519419c07ab5/68346dc23a374a7944af31adc5987294d2b1fd0d313fe8b32caf8f4ce8b1aff6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20241008%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241008T044548Z&X-Amz-Expires=86400&X-Amz-Signature=fe533b4490f5aa1d7e6027c06d4a393a5749d9fad17126920c296433dc19af25&X-Amz-SignedHeaders=host&partNumber=3&uploadId=Dz4n.pFEH3FoaxSDQuM3eaMH1LL3JsDVJPpjRAcdYD9E2ZQNUogR94xVvqpGzJgCasj2HXq7Ks3GJ_JL7tNPFxLzLjwrabuQRf9INIq0i3n6fwXkgEhRtpH9X8A7N92s&x-id=UploadPart (Caused by ProxyError('Unable to connect to proxy', RemoteDisconnected('Remote end closed connection without response')))"), '(Request ID: 57c85163-94af-4f6c-a7a0-c2f9bdae72d3)')' thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/fa/d9/fad929c7ce422826d6b5e86f6e73367545df88497599b59186ed519419c07ab5/68346dc23a374a7944af31adc5987294d2b1fd0d313fe8b32caf8f4ce8b1aff6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20241008%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241008T044548Z&X-Amz-Expires=86400&X-Amz-Signature=fe533b4490f5aa1d7e6027c06d4a393a5749d9fad17126920c296433dc19af25&X-Amz-SignedHeaders=host&partNumber=3&uploadId=Dz4n.pFEH3FoaxSDQuM3eaMH1LL3JsDVJPpjRAcdYD9E2ZQNUogR94xVvqpGzJgCasj2HXq7Ks3GJ_JL7tNPFxLzLjwrabuQRf9INIq0i3n6fwXkgEhRtpH9X8A7N92s&x-id=UploadPart
Retrying in 1s [Retry 1/5].
10/08/2024 13:11:09 - WARNING - huggingface_hub.utils._http - Retrying in 1s [Retry 1/5].
mp_rank_00_model_states.pt:  12%| | 400M/3.47G [24:5'(MaxRetryError("HTTPSConnectionPool(host='hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com', port=443): Max retries exceeded with url: /repos/fa/d9/fad929c7ce422826d6b5e86f6e73367545df88497599b59186ed519419c07ab5/e38013360af3b3dad0b1d61ebedb7c4da4e07064e2900adb63901ca0700c238b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20241008%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241008T044548Z&X-Amz-Expires=86400&X-Amz-Signature=45b709fe94e8a7ac97cd1d84f82a176ce553ed478347e26a4c3ec87b36f6894d&X-Amz-SignedHeaders=host&partNumber=25&uploadId=MfOXgP8vRG62J.ASy22Scj96qIsKoS1YL4VitPCtqdgL4yGfpQQlDMyfgqScNuJS9oR.rXlqiUrzzWg3YGJ43LhjoFRStFTqRG.PLRFhEda_b1Gm_6W5xty2tOw0CslB&x-id=UploadPart (Caused by ProxyError('Unable to connect to proxy', RemoteDisconnected('Remote end closed connection without response')))"), '(Request ID: d6984cef-860f-4852-8a8e-17dba4ae1b49)')' thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/fa/d9/fad929c7ce422826d6b5e86f6e73367545df88497599b59186ed519419c07ab5/e38013360af3b3dad0b1d61ebedb7c4da4e07064e2900adb63901ca0700c238b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20241008%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241008T044548Z&X-Amz-Expires=86400&X-Amz-Signature=45b709fe94e8a7ac97cd1d84f82a176ce553ed478347e26a4c3ec87b36f6894d&X-Amz-SignedHeaders=host&partNumber=25&uploadId=MfOXgP8vRG62J.ASy22Scj96qIsKoS1YL4VitPCtqdgL4yGfpQQlDMyfgqScNuJS9oR.rXlqiUrzzWg3YGJ43LhjoFRStFTqRG.PLRFhEda_b1Gm_6W5xty2tOw0CslB&x-id=UploadPart
10/08/2024 13:12:00 - WARNING - huggingface_hub.utils._http - '(MaxRetryError("HTTPSConnectionPool(host='hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com', port=443): Max retries exceeded with url: /repos/fa/d9/fad929c7ce422826d6b5e86f6e73367545df88497599b59186ed519419c07ab5/e38013360af3b3dad0b1d61ebedb7c4da4e07064e2900adb63901ca0700c238b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20241008%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241008T044548Z&X-Amz-Expires=86400&X-Amz-Signature=45b709fe94e8a7ac97cd1d84f82a176ce553ed478347e26a4c3ec87b36f6894d&X-Amz-SignedHeaders=host&partNumber=25&uploadId=MfOXgP8vRG62J.ASy22Scj96qIsKoS1YL4VitPCtqdgL4yGfpQQlDMyfgqScNuJS9oR.rXlqiUrzzWg3YGJ43LhjoFRStFTqRG.PLRFhEda_b1Gm_6W5xty2tOw0CslB&x-id=UploadPart (Caused by ProxyError('Unable to connect to proxy', RemoteDisconnected('Remote end closed connection without response')))"), '(Request ID: d6984cef-860f-4852-8a8e-17dba4ae1b49)')' thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/fa/d9/fad929c7ce422826d6b5e86f6e73367545df88497599b59186ed519419c07ab5/e38013360af3b3dad0b1d61ebedb7c4da4e07064e2900adb63901ca0700c238b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20241008%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241008T044548Z&X-Amz-Expires=86400&X-Amz-Signature=45b709fe94e8a7ac97cd1d84f82a176ce553ed478347e26a4c3ec87b36f6894d&X-Amz-SignedHeaders=host&partNumber=25&uploadId=MfOXgP8vRG62J.ASy22Scj96qIsKoS1YL4VitPCtqdgL4yGfpQQlDMyfgqScNuJS9oR.rXlqiUrzzWg3YGJ43LhjoFRStFTqRG.PLRFhEda_b1Gm_6W5xty2tOw0CslB&x-id=UploadPart
Retrying in 1s [Retry 1/5].  11%| | 384M/3.47G [25:5
10/08/2024 13:12:00 - WARNING - huggingface_hub.utils._http - Retrying in 1s [Retry 1/5].
                                                    '(MaxRetryError("HTTPSConnectionPool(host='hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com', port=443): Max retries exceeded with url: /repos/fa/d9/fad929c7ce422826d6b5e86f6e73367545df88497599b59186ed519419c07ab5/a44d967051d043a0287c94ba8c6a5298be6e402a30cb809fe0c66b68f1c30c9f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20241008%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241008T044549Z&X-Amz-Expires=86400&X-Amz-Signature=bc90df6f5327111d480083b3da30243aea4c69639977c174476bf245e3ad09ad&X-Amz-SignedHeaders=host&partNumber=23&uploadId=HxoRi0BjEk4pdGeUvIURsWrACnOL4uWykyNA5REk4w3Nh8g_vpq2K1mIq5bN6eNy2WmkfEZ8OK3.WAp90VBdPtKyMDQzu5UylIY5MXTUsRXTBDXzjat2g6xjK.ruxzea&x-id=UploadPart (Caused by ProxyError('Unable to connect to proxy', RemoteDisconnected('Remote end closed connection without response')))"), '(Request ID: fb3bb900-a5f5-45fd-b6ca-48f045df8675)')' thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/fa/d9/fad929c7ce422826d6b5e86f6e73367545df88497599b59186ed519419c07ab5/a44d967051d043a0287c94ba8c6a5298be6e402a30cb809fe0c66b68f1c30c9f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20241008%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241008T044549Z&X-Amz-Expires=86400&X-Amz-Signature=bc90df6f5327111d480083b3da30243aea4c69639977c174476bf245e3ad09ad&X-Amz-SignedHeaders=host&partNumber=23&uploadId=HxoRi0BjEk4pdGeUvIURsWrACnOL4uWykyNA5REk4w3Nh8g_vpq2K1mIq5bN6eNy2WmkfEZ8OK3.WAp90VBdPtKyMDQzu5UylIY5MXTUsRXTBDXzjat2g6xjK.ruxzea&x-id=UploadPart
10/08/2024 13:12:25 - WARNING - huggingface_hub.utils._http - '(MaxRetryError("HTTPSConnectionPool(host='hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com', port=443): Max retries exceeded with url: /repos/fa/d9/fad929c7ce422826d6b5e86f6e73367545df88497599b59186ed519419c07ab5/a44d967051d043a0287c94ba8c6a5298be6e402a30cb809fe0c66b68f1c30c9f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20241008%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241008T044549Z&X-Amz-Expires=86400&X-Amz-Signature=bc90df6f5327111d480083b3da30243aea4c69639977c174476bf245e3ad09ad&X-Amz-SignedHeaders=host&partNumber=23&uploadId=HxoRi0BjEk4pdGeUvIURsWrACnOL4uWykyNA5REk4w3Nh8g_vpq2K1mIq5bN6eNy2WmkfEZ8OK3.WAp90VBdPtKyMDQzu5UylIY5MXTUsRXTBDXzjat2g6xjK.ruxzea&x-id=UploadPart (Caused by ProxyError('Unable to connect to proxy', RemoteDisconnected('Remote end closed connection without response')))"), '(Request ID: fb3bb900-a5f5-45fd-b6ca-48f045df8675)')' thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/fa/d9/fad929c7ce422826d6b5e86f6e73367545df88497599b59186ed519419c07ab5/a44d967051d043a0287c94ba8c6a5298be6e402a30cb809fe0c66b68f1c30c9f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20241008%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241008T044549Z&X-Amz-Expires=86400&X-Amz-Signature=bc90df6f5327111d480083b3da30243aea4c69639977c174476bf245e3ad09ad&X-Amz-SignedHeaders=host&partNumber=23&uploadId=HxoRi0BjEk4pdGeUvIURsWrACnOL4uWykyNA5REk4w3Nh8g_vpq2K1mIq5bN6eNy2WmkfEZ8OK3.WAp90VBdPtKyMDQzu5UylIY5MXTUsRXTBDXzjat2g6xjK.ruxzea&x-id=UploadPart
Retrying in 1s [Retry 1/5].
10/08/2024 13:12:25 - WARNING - huggingface_hub.utils._http - Retrying in 1s [Retry 1/5].
mp_rank_00_model_states.pt:  12%| | 416M/3.47G [26:0'(MaxRetryError("HTTPSConnectionPool(host='hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com', port=443): Max retries exceeded with url: /repos/fa/d9/fad929c7ce422826d6b5e86f6e73367545df88497599b59186ed519419c07ab5/e38013360af3b3dad0b1d61ebedb7c4da4e07064e2900adb63901ca0700c238b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20241008%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241008T044548Z&X-Amz-Expires=86400&X-Amz-Signature=45b709fe94e8a7ac97cd1d84f82a176ce553ed478347e26a4c3ec87b36f6894d&X-Amz-SignedHeaders=host&partNumber=25&uploadId=MfOXgP8vRG62J.ASy22Scj96qIsKoS1YL4VitPCtqdgL4yGfpQQlDMyfgqScNuJS9oR.rXlqiUrzzWg3YGJ43LhjoFRStFTqRG.PLRFhEda_b1Gm_6W5xty2tOw0CslB&x-id=UploadPart (Caused by ProxyError('Unable to connect to proxy', RemoteDisconnected('Remote end closed connection without response')))"), '(Request ID: 40c8a93b-d2ce-4e97-9608-cca759f7f1ff)')' thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/fa/d9/fad929c7ce422826d6b5e86f6e73367545df88497599b59186ed519419c07ab5/e38013360af3b3dad0b1d61ebedb7c4da4e07064e2900adb63901ca0700c238b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20241008%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241008T044548Z&X-Amz-Expires=86400&X-Amz-Signature=45b709fe94e8a7ac97cd1d84f82a176ce553ed478347e26a4c3ec87b36f6894d&X-Amz-SignedHeaders=host&partNumber=25&uploadId=MfOXgP8vRG62J.ASy22Scj96qIsKoS1YL4VitPCtqdgL4yGfpQQlDMyfgqScNuJS9oR.rXlqiUrzzWg3YGJ43LhjoFRStFTqRG.PLRFhEda_b1Gm_6W5xty2tOw0CslB&x-id=UploadPart
10/08/2024 13:13:10 - WARNING - huggingface_hub.utils._http - '(MaxRetryError("HTTPSConnectionPool(host='hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com', port=443): Max retries exceeded with url: /repos/fa/d9/fad929c7ce422826d6b5e86f6e73367545df88497599b59186ed519419c07ab5/e38013360af3b3dad0b1d61ebedb7c4da4e07064e2900adb63901ca0700c238b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20241008%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241008T044548Z&X-Amz-Expires=86400&X-Amz-Signature=45b709fe94e8a7ac97cd1d84f82a176ce553ed478347e26a4c3ec87b36f6894d&X-Amz-SignedHeaders=host&partNumber=25&uploadId=MfOXgP8vRG62J.ASy22Scj96qIsKoS1YL4VitPCtqdgL4yGfpQQlDMyfgqScNuJS9oR.rXlqiUrzzWg3YGJ43LhjoFRStFTqRG.PLRFhEda_b1Gm_6W5xty2tOw0CslB&x-id=UploadPart (Caused by ProxyError('Unable to connect to proxy', RemoteDisconnected('Remote end closed connection without response')))"), '(Request ID: 40c8a93b-d2ce-4e97-9608-cca759f7f1ff)')' thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/fa/d9/fad929c7ce422826d6b5e86f6e73367545df88497599b59186ed519419c07ab5/e38013360af3b3dad0b1d61ebedb7c4da4e07064e2900adb63901ca0700c238b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20241008%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241008T044548Z&X-Amz-Expires=86400&X-Amz-Signature=45b709fe94e8a7ac97cd1d84f82a176ce553ed478347e26a4c3ec87b36f6894d&X-Amz-SignedHeaders=host&partNumber=25&uploadId=MfOXgP8vRG62J.ASy22Scj96qIsKoS1YL4VitPCtqdgL4yGfpQQlDMyfgqScNuJS9oR.rXlqiUrzzWg3YGJ43LhjoFRStFTqRG.PLRFhEda_b1Gm_6W5xty2tOw0CslB&x-id=UploadPart
Retrying in 2s [Retry 2/5].  12%| | 400M/3.47G [26:5
10/08/2024 13:13:10 - WARNING - huggingface_hub.utils._http - Retrying in 2s [Retry 2/5].
                                                    '(MaxRetryError("HTTPSConnectionPool(host='hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com', port=443): Max retries exceeded with url: /repos/fa/d9/fad929c7ce422826d6b5e86f6e73367545df88497599b59186ed519419c07ab5/19d04af0e3479ed569edf8736489cd0963626796f687a5297984208d5349fe06?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20241008%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241008T044548Z&X-Amz-Expires=86400&X-Amz-Signature=b0ff2db737dbc3b85a35f4b86cb9b92dc4d1ec43e8f4e892b05b64019888dd28&X-Amz-SignedHeaders=host&partNumber=25&uploadId=H65wfJNm4sJ_XSqjhzNgq86Z4NSDQ8DIFt_ycyyraNyYDRgoXOIb9F3W6RkksECTK1pjmGcIoDDCsKmumrU7VNBRwxHr2_ZBPffzh64npBrD7mU8R65KjehiSJNnCGIn&x-id=UploadPart (Caused by ProxyError('Unable to connect to proxy', RemoteDisconnected('Remote end closed connection without response')))"), '(Request ID: 0e3bebdb-73c9-496c-b9d2-29a79e8c4df7)')' thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/fa/d9/fad929c7ce422826d6b5e86f6e73367545df88497599b59186ed519419c07ab5/19d04af0e3479ed569edf8736489cd0963626796f687a5297984208d5349fe06?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20241008%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241008T044548Z&X-Amz-Expires=86400&X-Amz-Signature=b0ff2db737dbc3b85a35f4b86cb9b92dc4d1ec43e8f4e892b05b64019888dd28&X-Amz-SignedHeaders=host&partNumber=25&uploadId=H65wfJNm4sJ_XSqjhzNgq86Z4NSDQ8DIFt_ycyyraNyYDRgoXOIb9F3W6RkksECTK1pjmGcIoDDCsKmumrU7VNBRwxHr2_ZBPffzh64npBrD7mU8R65KjehiSJNnCGIn&x-id=UploadPart
10/08/2024 13:13:57 - WARNING - huggingface_hub.utils._http - '(MaxRetryError("HTTPSConnectionPool(host='hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com', port=443): Max retries exceeded with url: /repos/fa/d9/fad929c7ce422826d6b5e86f6e73367545df88497599b59186ed519419c07ab5/19d04af0e3479ed569edf8736489cd0963626796f687a5297984208d5349fe06?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20241008%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241008T044548Z&X-Amz-Expires=86400&X-Amz-Signature=b0ff2db737dbc3b85a35f4b86cb9b92dc4d1ec43e8f4e892b05b64019888dd28&X-Amz-SignedHeaders=host&partNumber=25&uploadId=H65wfJNm4sJ_XSqjhzNgq86Z4NSDQ8DIFt_ycyyraNyYDRgoXOIb9F3W6RkksECTK1pjmGcIoDDCsKmumrU7VNBRwxHr2_ZBPffzh64npBrD7mU8R65KjehiSJNnCGIn&x-id=UploadPart (Caused by ProxyError('Unable to connect to proxy', RemoteDisconnected('Remote end closed connection without response')))"), '(Request ID: 0e3bebdb-73c9-496c-b9d2-29a79e8c4df7)')' thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/fa/d9/fad929c7ce422826d6b5e86f6e73367545df88497599b59186ed519419c07ab5/19d04af0e3479ed569edf8736489cd0963626796f687a5297984208d5349fe06?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20241008%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241008T044548Z&X-Amz-Expires=86400&X-Amz-Signature=b0ff2db737dbc3b85a35f4b86cb9b92dc4d1ec43e8f4e892b05b64019888dd28&X-Amz-SignedHeaders=host&partNumber=25&uploadId=H65wfJNm4sJ_XSqjhzNgq86Z4NSDQ8DIFt_ycyyraNyYDRgoXOIb9F3W6RkksECTK1pjmGcIoDDCsKmumrU7VNBRwxHr2_ZBPffzh64npBrD7mU8R65KjehiSJNnCGIn&x-id=UploadPart
Retrying in 1s [Retry 1/5].
10/08/2024 13:13:57 - WARNING - huggingface_hub.utils._http - Retrying in 1s [Retry 1/5].
mp_rank_00_model_states.pt:  12%| | 432M/3.47G [27:1'(MaxRetryError("HTTPSConnectionPool(host='hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com', port=443): Max retries exceeded with url: /repos/fa/d9/fad929c7ce422826d6b5e86f6e73367545df88497599b59186ed519419c07ab5/a44d967051d043a0287c94ba8c6a5298be6e402a30cb809fe0c66b68f1c30c9f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20241008%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241008T044549Z&X-Amz-Expires=86400&X-Amz-Signature=bc90df6f5327111d480083b3da30243aea4c69639977c174476bf245e3ad09ad&X-Amz-SignedHeaders=host&partNumber=23&uploadId=HxoRi0BjEk4pdGeUvIURsWrACnOL4uWykyNA5REk4w3Nh8g_vpq2K1mIq5bN6eNy2WmkfEZ8OK3.WAp90VBdPtKyMDQzu5UylIY5MXTUsRXTBDXzjat2g6xjK.ruxzea&x-id=UploadPart (Caused by ProxyError('Unable to connect to proxy', RemoteDisconnected('Remote end closed connection without response')))"), '(Request ID: 78682937-dc7c-4b6f-960e-26a8c75840f6)')' thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/fa/d9/fad929c7ce422826d6b5e86f6e73367545df88497599b59186ed519419c07ab5/a44d967051d043a0287c94ba8c6a5298be6e402a30cb809fe0c66b68f1c30c9f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20241008%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241008T044549Z&X-Amz-Expires=86400&X-Amz-Signature=bc90df6f5327111d480083b3da30243aea4c69639977c174476bf245e3ad09ad&X-Amz-SignedHeaders=host&partNumber=23&uploadId=HxoRi0BjEk4pdGeUvIURsWrACnOL4uWykyNA5REk4w3Nh8g_vpq2K1mIq5bN6eNy2WmkfEZ8OK3.WAp90VBdPtKyMDQzu5UylIY5MXTUsRXTBDXzjat2g6xjK.ruxzea&x-id=UploadPart
10/08/2024 13:14:10 - WARNING - huggingface_hub.utils._http - '(MaxRetryError("HTTPSConnectionPool(host='hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com', port=443): Max retries exceeded with url: /repos/fa/d9/fad929c7ce422826d6b5e86f6e73367545df88497599b59186ed519419c07ab5/a44d967051d043a0287c94ba8c6a5298be6e402a30cb809fe0c66b68f1c30c9f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20241008%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241008T044549Z&X-Amz-Expires=86400&X-Amz-Signature=bc90df6f5327111d480083b3da30243aea4c69639977c174476bf245e3ad09ad&X-Amz-SignedHeaders=host&partNumber=23&uploadId=HxoRi0BjEk4pdGeUvIURsWrACnOL4uWykyNA5REk4w3Nh8g_vpq2K1mIq5bN6eNy2WmkfEZ8OK3.WAp90VBdPtKyMDQzu5UylIY5MXTUsRXTBDXzjat2g6xjK.ruxzea&x-id=UploadPart (Caused by ProxyError('Unable to connect to proxy', RemoteDisconnected('Remote end closed connection without response')))"), '(Request ID: 78682937-dc7c-4b6f-960e-26a8c75840f6)')' thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/fa/d9/fad929c7ce422826d6b5e86f6e73367545df88497599b59186ed519419c07ab5/a44d967051d043a0287c94ba8c6a5298be6e402a30cb809fe0c66b68f1c30c9f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20241008%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241008T044549Z&X-Amz-Expires=86400&X-Amz-Signature=bc90df6f5327111d480083b3da30243aea4c69639977c174476bf245e3ad09ad&X-Amz-SignedHeaders=host&partNumber=23&uploadId=HxoRi0BjEk4pdGeUvIURsWrACnOL4uWykyNA5REk4w3Nh8g_vpq2K1mIq5bN6eNy2WmkfEZ8OK3.WAp90VBdPtKyMDQzu5UylIY5MXTUsRXTBDXzjat2g6xjK.ruxzea&x-id=UploadPart
Retrying in 2s [Retry 2/5].
10/08/2024 13:14:10 - WARNING - huggingface_hub.utils._http - Retrying in 2s [Retry 2/5].
mp_rank_00_model_states.pt:  13%|▏| 439M/3.47G [28:0'(MaxRetryError("HTTPSConnectionPool(host='hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com', port=443): Max retries exceeded with url: /repos/fa/d9/fad929c7ce422826d6b5e86f6e73367545df88497599b59186ed519419c07ab5/19d04af0e3479ed569edf8736489cd0963626796f687a5297984208d5349fe06?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20241008%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241008T044548Z&X-Amz-Expires=86400&X-Amz-Signature=b0ff2db737dbc3b85a35f4b86cb9b92dc4d1ec43e8f4e892b05b64019888dd28&X-Amz-SignedHeaders=host&partNumber=25&uploadId=H65wfJNm4sJ_XSqjhzNgq86Z4NSDQ8DIFt_ycyyraNyYDRgoXOIb9F3W6RkksECTK1pjmGcIoDDCsKmumrU7VNBRwxHr2_ZBPffzh64npBrD7mU8R65KjehiSJNnCGIn&x-id=UploadPart (Caused by ProxyError('Unable to connect to proxy', RemoteDisconnected('Remote end closed connection without response')))"), '(Request ID: ca024bf9-d7d3-4434-be54-34feb7624e2d)')' thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/fa/d9/fad929c7ce422826d6b5e86f6e73367545df88497599b59186ed519419c07ab5/19d04af0e3479ed569edf8736489cd0963626796f687a5297984208d5349fe06?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20241008%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241008T044548Z&X-Amz-Expires=86400&X-Amz-Signature=b0ff2db737dbc3b85a35f4b86cb9b92dc4d1ec43e8f4e892b05b64019888dd28&X-Amz-SignedHeaders=host&partNumber=25&uploadId=H65wfJNm4sJ_XSqjhzNgq86Z4NSDQ8DIFt_ycyyraNyYDRgoXOIb9F3W6RkksECTK1pjmGcIoDDCsKmumrU7VNBRwxHr2_ZBPffzh64npBrD7mU8R65KjehiSJNnCGIn&x-id=UploadPart
10/08/2024 13:15:11 - WARNING - huggingface_hub.utils._http - '(MaxRetryError("HTTPSConnectionPool(host='hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com', port=443): Max retries exceeded with url: /repos/fa/d9/fad929c7ce422826d6b5e86f6e73367545df88497599b59186ed519419c07ab5/19d04af0e3479ed569edf8736489cd0963626796f687a5297984208d5349fe06?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20241008%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241008T044548Z&X-Amz-Expires=86400&X-Amz-Signature=b0ff2db737dbc3b85a35f4b86cb9b92dc4d1ec43e8f4e892b05b64019888dd28&X-Amz-SignedHeaders=host&partNumber=25&uploadId=H65wfJNm4sJ_XSqjhzNgq86Z4NSDQ8DIFt_ycyyraNyYDRgoXOIb9F3W6RkksECTK1pjmGcIoDDCsKmumrU7VNBRwxHr2_ZBPffzh64npBrD7mU8R65KjehiSJNnCGIn&x-id=UploadPart (Caused by ProxyError('Unable to connect to proxy', RemoteDisconnected('Remote end closed connection without response')))"), '(Request ID: ca024bf9-d7d3-4434-be54-34feb7624e2d)')' thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/fa/d9/fad929c7ce422826d6b5e86f6e73367545df88497599b59186ed519419c07ab5/19d04af0e3479ed569edf8736489cd0963626796f687a5297984208d5349fe06?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20241008%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241008T044548Z&X-Amz-Expires=86400&X-Amz-Signature=b0ff2db737dbc3b85a35f4b86cb9b92dc4d1ec43e8f4e892b05b64019888dd28&X-Amz-SignedHeaders=host&partNumber=25&uploadId=H65wfJNm4sJ_XSqjhzNgq86Z4NSDQ8DIFt_ycyyraNyYDRgoXOIb9F3W6RkksECTK1pjmGcIoDDCsKmumrU7VNBRwxHr2_ZBPffzh64npBrD7mU8R65KjehiSJNnCGIn&x-id=UploadPart
Retrying in 2s [Retry 2/5].  12%| | 416M/3.47G [28:5
10/08/2024 13:15:11 - WARNING - huggingface_hub.utils._http - Retrying in 2s [Retry 2/5].
Upload 151 LFS files:   1%| | 1/151 [30:17<75:44:42,
mp_rank_00_model_states.pt:  12%| | 430M/3.47G [29:1
mp_rank_00_model_states.pt:  12%| | 431M/3.47G [30:1
mp_rank_00_model_states.pt:   3%| | 106M/3.47G [08:1
mp_rank_00_model_states.pt:  13%|▏| 461M/3.47G [29:5
mp_rank_00_model_states.pt:  13%|▏| 448M/3.47G [29:4

mp_rank_00_model_states.pt:   3%| | 112M/3.47G [08:3
